import boto3
import logging
import io

from third_lambda_utils.third_lambda_init                import third_lambda_init
from third_lambda_utils.make_insert_queries_from_parquet import make_insert_queries_from_parquet
from third_lambda_utils.make_SQL_queries_to_warehouse    import make_SQL_queries_to_warehouse
from third_lambda_utils.conn_to_db                       import conn_to_db, close_db
from third_lambda_utils.errors_lookup                    import errors_lookup
from third_lambda_utils.info_lookup                      import info_lookup



root_logger = logging.getLogger()

# Create and configure a logger 
# that writes to a file:
logging.basicConfig(
    level=logging.DEBUG,                                         # Log level (includes INFO, WARNING, ERROR)
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",  # Log format
    filemode="a"                                                 # 'a' appends. 'w' would overwrite
                   )





def third_lambda_handler(event, context):
    """
    This function:
        1) receives an event object from 
            the processed bucket when the 
            second lambda function stores 
            a dimension table or the fact 
            table there as a Parquet file
            in a buffer.
        2) gets the Parquet file from the
            the processed bucket and 
            creates a list of SQL queries
            where each query will write 
            one row of the table in the 
            Parquet file into the data 
            warehouse.
        3) connects to the data warehouse 
            and loops through the list of 
            query strings to make the 
            queries to the warehouse.
            The query strings for dimension 
            tables ensure updated row data 
            replaces outdated row data in the 
            warehouse. Query strings for the
            fact_sales_order table, in 
            contrast, add updated rows to 
            the fact_sales_order table in 
            the warehouse, ensuring the 
            previous rows remain.
        4) closes the connection to the 
            warehouse.            

    Args:
        event: object generated by AWS 
         Lambda when the second lambda 
         handler writes a Parquet file 
         (that represents a whole table
         or only updated rows) to the 
         processed bucket.

        context: metadata about the 
         environment in which this 
         lambda function runs.

    Returns:
        None                            
    """
    # log the status:
    root_logger.info(info_lookup['info_0'])        


    # Get lookup table that contains 
    # values this lambda handler requires:
    # lookup['proc_bucket'] -- name of processed bucket
    # lookup['s3_client']      -- boto3 S3 client object
    # lookup['object_key']    -- key under which processed bucket saved Parquet file
    # lookup['table_name']    -- name of table
    # lookup['conn']                -- pg8000.native Connection object that knows about warehouse
    lookup = third_lambda_init(event, conn_to_db, close_db, boto3.client('s3'))   


    try:
        # Get the buffer that contains 
        # the Parquet file that represents
        # the dimension table/facts table:
        dict_from_s3 = lookup['s3_client'].get_object(Key=lookup['object_key'], Bucket=lookup['proc_bucket'])
        strmng_bod_obj = dict_from_s3["Body"]
        raw_bytes = strmng_bod_obj.read()
        pq_buff = io.BytesIO(raw_bytes) 
    except Exception:
        # log the exception 
        # and stop the code:
        root_logger.error(errors_lookup['err_0'] + f'{lookup['table_name']}')
        raise


    # create SQL query strings from
    # the data in the Parquet file 
    # in the buffer. Each query string 
    # is for inserting one row of a 
    # table into that table in the 
    # warehouse:
    queries_list = make_insert_queries_from_parquet(pq_buff, lookup['table_name'])
    


    # Use the SQL queries to put
    # table data into the 
    # warehouse:
    make_SQL_queries_to_warehouse(queries_list, lookup['conn'])  

    # Close connection to warehouse:
    close_db(lookup['conn'])

    # log the status:
    root_logger.info(info_lookup['info_1'] + f'{lookup['table_name']}')


