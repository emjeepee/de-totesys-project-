import boto3
import logging
import io

from botocore.exceptions import ClientError


from src.third_lambda.third_lambda_utils.third_lambda_init                import third_lambda_init
from src.third_lambda.third_lambda_utils.make_insert_queries_from_parquet import make_insert_queries_from_parquet
from src.third_lambda.third_lambda_utils.make_SQL_queries_to_warehouse    import make_SQL_queries_to_warehouse
from src.third_lambda.third_lambda_utils.conn_to_db                       import conn_to_db, close_db
from src.third_lambda.third_lambda_utils.errors_lookup                    import errors_lookup
from src.third_lambda.third_lambda_utils.info_lookup                      import info_lookup
from src.third_lambda.third_lambda_utils.get_inbuffer_parquet             import get_inbuffer_parquet 


root_logger = logging.getLogger()

# Create and configure a logger 
# that writes to a file:
logging.basicConfig(
    level=logging.DEBUG, # log anything above level DEBUG, the lowest level  
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s", 
    filemode="a"  
                   )





def third_lambda_handler(event, context):
    """
    This function:
        1) receives an event object from 
            the processed bucket when the 
            second lambda function stores 
            a dimension table or the fact 
            table there as a Parquet file
            in a buffer.
        2) gets the Parquet file from the
            the processed bucket and 
            creates a list of SQL queries
            where each query will write 
            one row of the table in the 
            Parquet file into the data 
            warehouse.
        3) connects to the data warehouse 
            and loops through the list of 
            query strings to make the 
            queries to the warehouse.
            The query strings for dimension 
            tables ensure updated row data 
            replaces outdated row data in the 
            warehouse. Query strings for the
            fact_sales_order table, in 
            contrast, add updated rows to 
            the fact_sales_order table in 
            the warehouse, ensuring the 
            previous rows remain.
        4) closes the connection to the 
            warehouse.            

    Args:
        event: object generated by AWS 
         Lambda when the second lambda 
         handler writes a Parquet file 
         (that represents a whole table
         or only updated rows) to the 
         processed bucket.

        context: metadata about the 
         environment in which this 
         lambda function runs.

    Returns:
        None                            
    """
    # log the status:
    root_logger.info(info_lookup['info_0'])        

    # Get lookup table that contains 
    # values this lambda handler requires:
    # lookup['proc_bucket'] -- name of processed bucket
    # lookup['s3_client']      -- boto3 S3 client object
    # lookup['object_key']    -- key under which processed bucket saved Parquet file
    # lookup['table_name']    -- name of table
    # lookup['conn']                -- pg8000.native Connection object that knows about warehouse
    lookup = third_lambda_init(event, conn_to_db, close_db, boto3.client('s3'))   


    # Get the buffer that contains 
    # the Parquet file that represents
    # the dimension table/facts table:
    pq_buff = get_inbuffer_parquet(lookup['s3_client'],
                             lookup['object_key'],
                             lookup['proc_bucket'],
                             lookup['table_name']
                             )


    # create SQL query strings from
    # the data in the Parquet file 
    # in the buffer. Each query string 
    # is for inserting one row of a 
    # table into that table in the 
    # warehouse:
    queries_list = make_insert_queries_from_parquet(pq_buff, lookup['table_name'])

    # Use the SQL queries to put
    # table data into the 
    # warehouse:
    make_SQL_queries_to_warehouse(queries_list, lookup['conn'])  

    # Close connection to warehouse:
    close_db(lookup['conn'])

    # log the status:
    root_logger.info(info_lookup['info_1'] + f'{lookup['table_name']}')


